{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penalties and Log Barriers\n",
    "\n",
    "Based on Homework at https://www.user.tu-berlin.de/mtoussai/teaching/13-Optimization/\n",
    "From M. Toussaint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Plots.PlotlyBackend()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Plots\n",
    "using LinearAlgebra\n",
    "\n",
    "plotly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Equality Constraint Penalties and augmented Lagrangian\n",
    "\n",
    "(We don't need to know what the Langangian is (yet) to solving this exercise.) In the lecture we discussed the squared penalty method for inequality constraints. There is a straight-forward version for equality constraints: Instead of\n",
    "$$\\min _{x} f(x) \\quad \\text { s.t. } \\quad h(x)=0 $$\n",
    "we address\n",
    "$$ \\min _{x} f(x)+\\mu \\sum_{i=1}^{m} h_{i}(x)^{2} $$\n",
    "such that the squared penalty pulls the solution onto the constraint $h(x)=0 .$ Assume that if we minimize (2) we end up at a solution $x_{1}$ for which each $h_{i}\\left(x_{1}\\right)$ is reasonable small, but not exactly zero. We also mentioned the idea that we could add an additional term which counteracts the violation of the constraint. This can be realized by minimizing\n",
    "$$ \\min _{x} f(x)+\\mu \\sum_{i=1}^{m} h_{i}(x)^{2}+\\sum_{i=1}^{m} \\lambda_{i} h_{i}(x) $$\n",
    "for a \"good choice\" of each $\\lambda_{i}$. It turns we can infer this \"good choice\" from the solution $x_{1}$ of (2) \n",
    "\n",
    "#### Task:\n",
    "Prove that setting $\\lambda_{i}=2 \\mu h_{i}\\left(x_{1}\\right)$ will, if we assume that the gradients $\\nabla f(x)$ and $\\nabla h(x)$ are (locally) constant, ensure that the minimum of (3) fulfils exactly the constraints $h(x)=0$ Tip: Think intuitive. Think about how the gradient that arises from the penalty in (2) is now generated via the $\\lambda_{i}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Squared Penalties \\& Log Barriers\n",
    "\n",
    "In the last exercise we defined the \"hole function\" $f_{\\text {hole }}^{c}(x),$ where we now assume a conditioning $c=4$ Consider the optimization problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.5",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
